# -*- coding: utf-8 -*-
"""RAG with knowledge graph Neo4j.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12uM8qBlOeJ5PGW-O_j1Np1P9gTeHqtkG
"""

!pip install --upgrade --quiet langchain langchain-community langchain-openai langchain-experimental neo4j wikipedia tiktoken

!pip install torch==2.6.0+cu124 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
!pip install fsspec==2025.3.2
!pip install langchain-neo4j

NEO4J_URI="neo4j+s://df03aa03.databases.neo4j.io"
NEO4J_USERNAME="neo4j"
NEO4J_PASSWORD="6uW9Cc7RloLPy6TIJimK4YUn196z8R5Wh7-dSCdeUF4"

from google.colab import userdata
OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')

import os

os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY
os.environ['NEO4J_URI'] = NEO4J_URI
os.environ['NEO4J_USERNAME'] = NEO4J_USERNAME
os.environ['NEO4J_PASSWORD'] = NEO4J_PASSWORD

from langchain_community.graphs import Neo4jGraph

graph = Neo4jGraph()

from langchain.document_loaders import WikipediaLoader
raw_docs = WikipediaLoader(query='Elizabeth I').load()

raw_docs

from langchain.text_splitter import CharacterTextSplitter
text_splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=24)
docs = text_splitter.split_documents(raw_docs[:3])

from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model_name='gpt-3.5-turbo')

from langchain_experimental.graph_transformers import LLMGraphTransformer

llm_transformer = LLMGraphTransformer(
    llm=llm
)

graph_documents = llm_transformer.convert_to_graph_documents(documents=docs)

graph_documents

graph.add_graph_documents(
    graph_documents=graph_documents,
    baseEntityLabel=True,
    include_source=True
)

default_cypher = "MATCH (s)-[r:!MENTIONS]->(t) RETURN s,r,t LIMIT 50"

default_cypher

try:
  import google.colab
  from google.colab import output
  output.enable_custom_widget_manager()
except:
  pass

!pip install yfiles_jupyter_graphs

from yfiles_jupyter_graphs import GraphWidget
from neo4j import GraphDatabase

def show_graph(cypher: str = default_cypher):
  driver = GraphDatabase.driver(
      uri = os.environ['NEO4J_URI'],
      auth = (os.environ['NEO4J_USERNAME'], os.environ['NEO4J_PASSWORD'])
  )
  session = driver.session()
  widget = GraphWidget(graph = session.run(cypher).graph())
  widget.node_label_mapping = 'id'
  display(widget)
  return widget

show_graph()

from langchain_community.vectorstores import Neo4jVector
from typing import List, Tuple, Optional

from langchain_openai import OpenAIEmbeddings

vector_index = Neo4jVector.from_existing_graph(
    OpenAIEmbeddings(),
    search_type = "hybrid",
    node_label = "Document",
    text_node_properties = ["text"],
    embedding_node_property = "embedding"
)

from langchain_core.pydantic_v1 import BaseModel, Field

class Entities(BaseModel):
  """Identifying information about the entities."""

  names: List[str] = Field(
      ...,
      description="All the person, organisation, or business entities that appear in the given text",
  )

from langchain.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are extracting organisation and person entities from text."
        ),
        (
            "human",
            "Use the given format to extract the information from the text."
            "input: {question}",
        ),
    ]
)

entity_chain = prompt | llm.with_structured_output(Entities)

entity_chain.invoke({"question": "Where was Amelia Earhart born?"}).names

from langchain_neo4j.vectorstores.neo4j_vector import remove_lucene_chars

def generate_full_text_query(input: str) -> str:
  full_text_query = ""
  words = [el for el in remove_lucene_chars(input).split(" ") if el]
  for word in words[:-1]:
    full_text_query += f"{word}~2 AND "
  full_text_query += f"{words[-1]}~2"
  return full_text_query.strip()

existing_indexes = graph.query("SHOW INDEXES YIELD name RETURN name")
if not any("entity" in idx["name"] for idx in existing_indexes):
    graph.query("""
        CREATE FULLTEXT INDEX entity FOR (n:Entity) ON EACH [n.name, n.description]
    """)

def structured_retriever(question: str) -> str:
    result = ""
    entities = entity_chain.invoke({"question": question})

    for entity in entities.names:
        response = graph.query(
            """
            CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})
            YIELD node, score
            CALL (node) {
              WITH node
              MATCH (node)-[r:!MENTIONS]->(neighbor)
              RETURN node.id + ' - ' + type(r) + ' - ' + neighbor.id AS output
              UNION ALL
              WITH node
              MATCH (node)<-[r:!MENTIONS]-(neighbor)
              RETURN node.id + ' - ' + type(r) + ' - ' + neighbor.id AS output
            }
            RETURN output LIMIT 50
            """,
            {"query": generate_full_text_query(entity)}
        )
        result += "\n".join([el["output"] for el in response]) + "\n"

    return result

print(structured_retriever("Who is Elizabeth I?"))

def retriever(question: str) -> str:
  print(f"Search query: {question}")
  structured_data = structured_retriever(question)
  unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]

  final_data = f"""Structured Data:
  {structured_data}
  Unstructured Data:
  {"#Document ".join(unstructured_data)}"""

  return final_data

_template = """Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.

Chat History:
{chat_history}
Follow Up Input: {question}
Standalone question:

"""

from langchain.prompts import PromptTemplate

CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)

def _format_chat_history(chat_history: List[Tuple[str, str]]) -> str:
  buffer = []
  for human, ai in chat_history:
    buffer.append(HumanMessage(content=human))
    buffer.append(AIMessage(content=ai))
  return buffer

from langchain_core.runnables import RunnableBranch, RunnableLambda, RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser

_search_query = RunnableBranch(
    (
        RunnableLambda(lambda x: bool(x.get("chat_history"))).with_config(
            run_name = "HasChatHistoryCheck"
        ),
        RunnablePassthrough.assign(
            chat_history = lambda x: _format_chat_history(x["chat_history"])
        )
        | CONDENSE_QUESTION_PROMPT
        | ChatOpenAI(temperature=0)
        | StrOutputParser(),
        ),
    RunnableLambda(lambda x: x["question"]),
    )

template = """Answer the question based only on the following context:
{context}

Question: {question}
Use natural language to answer the question.
"""

prompt = ChatPromptTemplate.from_template(template)

chain = (
    RunnableParallel(
        {
            "context": _search_query | retriever,
            "question": RunnablePassthrough(),
        }
    )
    | prompt
    | llm
    | StrOutputParser()

)

chain.invoke({"question": "Which house did Elizabeth I belong to?"})

